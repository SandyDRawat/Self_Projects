{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "0uU2425oPOZm",
        "VPIghmRJNWLW",
        "cd8ktabdNlzv",
        "lJMOBNsNN0GQ",
        "6WeFWEWwPwXk"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "0uU2425oPOZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth evaluate transformers nltk trl bitsandbytes peft\n",
        "!pip install sympy --upgrade\n",
        "!pip install llama-cpp-python\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "E1RHMxgXASTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary dependencies\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "from llama_cpp import Llama"
      ],
      "metadata": {
        "id": "iRcCt6YUASRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Huggingface to interact and push the models\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "# Authenticate using Hugging Face token for accessing gated models and upload the model to hub\n",
        "hf_token = userdata.get(\"hf_token\")\n",
        "login(token=hf_token)"
      ],
      "metadata": {
        "id": "19_HO79IB7Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training"
      ],
      "metadata": {
        "id": "VPIghmRJNWLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained base model using the FastLanguageModel as was mentioned in unsloth nnotebook\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    max_seq_length = 512,\n",
        "    dtype = None,          # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit = True,   # Use 4bit quantization to reduce memory usage. Can be False.\n",
        ")"
      ],
      "metadata": {
        "id": "Wf-c7wV4P1n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure and apply PEFT (Parameter-Efficient Fine-Tuning) to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 12\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "                      # only unfreezes these parameters for training, can also add more based on your model\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "53IOsiPaASNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined a input prompt for chinese translation purpose\n",
        "translation_prompt = \"\"\"Below is a Chinese text that needs to be translated into English.\n",
        "\n",
        "### Chinese Text:\n",
        "{}\n",
        "\n",
        "### English Translation:\n",
        "{}\"\"\"\n",
        "# Placeholder '{}' is used for formatting the Chinese text and English translation\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "# this function takes dataset with chinese and english columns, creates a translation prompt then saves them to text column\n",
        "    chinese = examples[\"Chinese\"]\n",
        "    english = examples[\"English\"]\n",
        "    texts = []\n",
        "    wtexts = []\n",
        "    for chinese, english in zip(chinese, english):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = translation_prompt.format(chinese, english) + EOS_TOKEN    # this is prompt with both english and chinese texts, used for training\n",
        "        wtext = translation_prompt.format(chinese, \"\")                    # this is prompt with chinese text only, used for inferencing and evaluation\n",
        "        texts.append(text)\n",
        "        wtexts.append(wtext)\n",
        "    return { \"text\" : texts, \"wtext\" : wtexts}                            # Add both texts to the dataset\n"
      ],
      "metadata": {
        "id": "Oe-wIDx2ASKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the custom dataset\n",
        "dataset = pd.read_csv(\"/content/translations_dataset.csv\")\n",
        "\n",
        "# Sample 45000 rows for processing (adjust as needed for system capacity)\n",
        "dataset = dataset.sample(n=45000, random_state=42)\n",
        "\n",
        "# Split the dataset into training and evaluation datasets (80-20 split)\n",
        "train_df, eval_df = train_test_split(dataset, test_size=0.01, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Apply formatting function to both datasets\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Output dataset sizes\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "id": "6cZXNu2eASG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "peto01wmBwki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512,\n",
        "    dataset_num_proc = 4,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 40,\n",
        "        gradient_accumulation_steps = 3,\n",
        "        warmup_steps = 5,\n",
        "        #num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        max_steps = 170,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "-Jv3qq_TASDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "sCkfSGqeASAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    translation_prompt.format(\n",
        "        \"尤其，这是孟浩在第七命下展开的七婴图腾，威力之强，轰天撼地。\", # chinese\n",
        "        \"\", # english - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "8ycjP79yAR8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and Upload the Model"
      ],
      "metadata": {
        "id": "cd8ktabdNlzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the final model as LoRA adapters, either use Huggingface's push_to_hub for an online save or save_pretrained for a local save.\n",
        "\n",
        "[NOTE] This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ],
      "metadata": {
        "id": "uJM-vR5HDjGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally and on hub as GGUF (8-bit Q8_0 quantization)\n",
        "# change the model as the correct version\n",
        "model.save_pretrained(\"llama3.1_zhtoen_translation_v4\") # Local saving\n",
        "tokenizer.save_pretrained(\"llama3.1_zhtoen_translation_v4\")\n",
        "\n",
        "model.push_to_hub(\"Rawsand/llama3.1_zhtoen_translation_v4\") # Online saving\n",
        "tokenizer.push_to_hub(\"Rawsand/llama3.1_zhtoen_translation_v4\") # Online saving"
      ],
      "metadata": {
        "id": "eXscwUQbAR5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally and on hub as 16bit\n",
        "model.save_pretrained_merged(\"model_name\", tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(\"username/model_name\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
      ],
      "metadata": {
        "id": "j1UJUAdhDxii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally and on hub as GGUF (8-bit Q8_0 quantization)\n",
        "model.save_pretrained_gguf(\"llama3.1_zhtoen_translation_v4_gguf\", tokenizer)\n",
        "model.push_to_hub_gguf(\"Rawsand/llama3.1_zhtoen_translation_v4_gguf\", tokenizer)"
      ],
      "metadata": {
        "id": "nRzFKMX5AR1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "lJMOBNsNN0GQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If  you did not train the model you can just call the previous version\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Rawsand/llama3.1_zhtoen_translation_v4\",\n",
        "    max_seq_length = 512,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "model = FastLanguageModel.for_inference(model)"
      ],
      "metadata": {
        "id": "p4Mz8NK7K9GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined a input prompt for chinese translation purpose\n",
        "translation_prompt = \"\"\"Below is a Chinese text that needs to be translated into English.\n",
        "\n",
        "### Chinese Text:\n",
        "{}\n",
        "\n",
        "### English Translation:\n",
        "{}\"\"\"\n",
        "# Placeholder '{}' is used for formatting the Chinese text and English translation\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "# this function takes dataset with chinese and english columns, creates a translation prompt then saves them to text column\n",
        "    chinese = examples[\"Chinese\"]\n",
        "    english = examples[\"English\"]\n",
        "    texts = []\n",
        "    wtexts = []\n",
        "    for chinese, english in zip(chinese, english):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = translation_prompt.format(chinese, english) + EOS_TOKEN    # this is prompt with both english and chinese texts, used for training\n",
        "        wtext = translation_prompt.format(chinese, \"\")                    # this is prompt with chinese text only, used for inferencing and evaluation\n",
        "        texts.append(text)\n",
        "        wtexts.append(wtext)\n",
        "    return { \"text\" : texts, \"wtext\" : wtexts}                            # Add both texts to the dataset"
      ],
      "metadata": {
        "id": "GIsCGDqIneFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the custom dataset\n",
        "dataset = pd.read_csv(\"/content/translations_dataset.csv\")\n",
        "\n",
        "# Sample 45000 rows for processing (adjust as needed for system capacity)\n",
        "dataset = dataset.sample(n=45000, random_state=42)\n",
        "\n",
        "# Split the dataset into training and evaluation datasets (80-20 split)\n",
        "train_df, eval_df = train_test_split(dataset, test_size=0.01, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# Apply formatting function to both datasets\n",
        "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
        "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# Output dataset sizes\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Evaluation dataset size: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "id": "fZ3Lde7-Q6pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming `validation_dataset` is your Hugging Face Dataset object\n",
        "inputs = eval_dataset[\"wtext\"]      # Get the Chinese only text prompts\n",
        "references = eval_dataset[\"text\"]   # Get the English and chinese text prompt as references, which are considered as desired output\n",
        "\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "for text in inputs:\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")                    # tokenize the output and load it to gpu\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)  # generate the output labels\n",
        "    prediction = tokenizer.batch_decode(outputs)                                # detokenize the labels to output\n",
        "    predictions.append(prediction)"
      ],
      "metadata": {
        "id": "0wnUNTe0LDWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "bleu_scores = []\n",
        "for ref, pred in zip(references, predictions):\n",
        "    # Tokenize the reference and prediction\n",
        "    reference_tokens = nltk.word_tokenize(ref)        # Tokenize the reference\n",
        "    prediction_tokens = nltk.word_tokenize(pred[0])   # Tokenize the prediction\n",
        "\n",
        "    bleu_score = sentence_bleu([reference_tokens], prediction_tokens) # get the bleu score\n",
        "    bleu_scores.append(bleu_score)\n",
        "\n",
        "# Print results\n",
        "for bleu in bleu_scores:\n",
        "    print(f\"BLEU Score: {bleu:.4f}\")\n",
        "\n",
        "# Calculate average BLEU score\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"Average BLEU Score: {average_bleu:.4f}\")"
      ],
      "metadata": {
        "id": "qwThZjmbLHr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inferencing using Llama-cpp"
      ],
      "metadata": {
        "id": "6WeFWEWwPwXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Llama model from the pre-trained weights\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"Rawsand/llama3.1_zhtoen_translation_v4_gguf\",  # Hugging Face repository ID for the pre-trained model\n",
        "    filename=\"unsloth.Q8_0.gguf\",  # The filename of the quantized model file\n",
        ")"
      ],
      "metadata": {
        "id": "b_NUCUgvWLQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the translation prompt template for translating Chinese text to English\n",
        "translation_prompt = \"\"\"Below is a Chinese text that needs to be translated into English.\n",
        "\n",
        "### Chinese Text:\n",
        "{}\n",
        "\n",
        "### English Translation:\n",
        "{}\"\"\"  # Placeholder '{}' is used for formatting the Chinese text and English translation\n",
        "\n",
        "# Function to format prompts for translation, without providing the English translation initially\n",
        "def formatting_prompts_func_without_english(chinese_texts):\n",
        "    \"\"\"\n",
        "    Format Chinese text inputs into the translation prompt template.\n",
        "\n",
        "    Args:\n",
        "        chinese_texts (list): List of Chinese text strings to be translated.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of formatted prompt strings ready for input to the LLM.\n",
        "    \"\"\"\n",
        "    texts = []  # Initialize an empty list to store formatted prompts\n",
        "    for chinese_text in chinese_texts:  # Iterate over each Chinese text\n",
        "        # Format the prompt by filling in the Chinese text, leaving the English translation blank\n",
        "        text = translation_prompt.format(chinese_text, \"\")\n",
        "        texts.append(text)  # Add the formatted prompt to the list\n",
        "    return texts  # Return the list of formatted prompts"
      ],
      "metadata": {
        "id": "uaWjEOJXWJPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Chinese texts to be translated\n",
        "texts = [\n",
        "    \"你的模型可以翻译中文吗？\",  # \"Can your model translate Chinese?\"\n",
        "    \"在这面具出现的刹那，忽然的，皮冻那里猛地睁大了眼，拍打着翅膀飞起，绕着孟浩飞了几圈，口中传出聒噪之声。\"  # A more complex Chinese sentence\n",
        "]"
      ],
      "metadata": {
        "id": "MBO3d4XaWRro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the formatting function to prepare the prompts\n",
        "prompt = formatting_prompts_func_without_english(texts)\n",
        "\n",
        "# Generate the output for the first prompt using the pre-trained model\n",
        "output = llm(\n",
        "    prompt[0],\n",
        "    max_tokens=512,  # Maximum number of tokens to generate in the output\n",
        "    echo=True  # Include the prompt in the output for reference\n",
        ")\n",
        "\n",
        "# print the response\n",
        "output['choices'][0]['text']"
      ],
      "metadata": {
        "id": "xswwGB7HWUR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}